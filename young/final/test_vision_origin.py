#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from my_robot_interfaces.msg import DetectionResult
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import numpy as np
import os
import torch
import torch.nn.functional as F
from torchvision.models import wide_resnet50_2, Wide_ResNet50_2_Weights
from PIL import Image as PILImage
from torchvision import transforms

# ==============================================================================
# [ì„¤ì • 1] ê²½ë¡œ ë° PaDiM íŒŒë¼ë¯¸í„°
# ==============================================================================
WEIGHTS_PATH = '/home/young/final_ws/src/final/final/padim_weights/cube'
NUM_RANDOM_CHANNELS = 1500
TOP_N_PATCHES = 10 
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ANOMALY_THRESHOLD = 170.0 

# ==============================================================================
# [ì„¤ì • 2] ROI ë° ì¢Œí‘œ ë³€í™˜ í–‰ë ¬
# ==============================================================================
# âš ï¸ ì¢Œí‘œ ë³€í™˜ í–‰ë ¬ (ì™œê³¡ ë³´ì •ëœ ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë§µí•‘í•œ ê°’ì´ì–´ì•¼ ì •í™•í•¨)
PERSPECTIVE_MATRIX = np.array([
    [-0.06894,  0.13738,  21.38340],
    [ 0.22404, -1.59558, 322.14769],
    [ 0.00160, -0.00696,   1.00000]
], dtype=np.float32)

ROI_X = 420; ROI_Y = 130; ROI_W = 300; ROI_H = 350

# ==============================================================================
# [ì„¤ì • 3] ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê°’ (ì  ì¢Œí‘œ ë³´ì •ìš©)
# ==============================================================================
# âš ï¸ ì‹¤ì œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê°’ìœ¼ë¡œ ê¼­ ì±„ì›Œì£¼ì„¸ìš”! (ì•„ë˜ëŠ” ì˜ˆì‹œ)
CAMERA_MATRIX = np.array([
    [1000.0,    0.0, 640.0],
    [   0.0, 1000.0, 360.0],
    [   0.0,    0.0,   1.0]
])
DIST_COEFFS = np.array([0.0, 0.0, 0.0, 0.0, 0.0]) 

# ìƒ‰ìƒ ë²”ìœ„ (ì´ˆë¡ìƒ‰ë§Œ)
LOWER_GREEN = np.array([35, 20, 20])
UPPER_GREEN = np.array([90, 255, 255])

class VisionNode(Node):
    def __init__(self):
        super().__init__('vision_node')
        
        self.result_pub = self.create_publisher(DetectionResult, '/vision/result', 10)
        self.image_pub = self.create_publisher(Image, '/vision/defect_img', 10)
        self.bridge = CvBridge()

        self.get_logger().info(f"ğŸš€ Vision Node Started. Using Device: {DEVICE}")

        self.load_padim_model()

        self.cap = cv2.VideoCapture(2)
        if not self.cap.isOpened():
            self.get_logger().warn("âš ï¸ Camera 2 failed. Switching to Camera 0...")
            self.cap = cv2.VideoCapture(0)

        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 960)

        self.timer = self.create_timer(0.03, self.process_frame)

    def load_padim_model(self):
        self.get_logger().info(f"ğŸ§  Loading PaDiM weights from: {WEIGHTS_PATH}")
        try:
            if not os.path.exists(os.path.join(WEIGHTS_PATH, 'mean_vector.npy')):
                raise FileNotFoundError(f"Weight files not found at {WEIGHTS_PATH}")

            mean_vec_np = np.load(os.path.join(WEIGHTS_PATH, 'mean_vector.npy'))
            inv_cov_np = np.load(os.path.join(WEIGHTS_PATH, 'inv_cov_matrix.npy'))
            self.random_channels = np.load(os.path.join(WEIGHTS_PATH, 'random_channels.npy'))

            self.mean_vector = torch.from_numpy(mean_vec_np).to(DEVICE).float()
            self.inv_cov_matrix = torch.from_numpy(inv_cov_np).to(DEVICE).float()

            self.model = wide_resnet50_2(weights=Wide_ResNet50_2_Weights.IMAGENET1K_V2)
            self.feature_maps = {}

            def hook_fn(module, input, output, name):
                if name == 'layer3':
                    output = F.interpolate(output, size=(28, 28), mode='bilinear', align_corners=False)
                elif name == 'layer1':
                    output = F.avg_pool2d(output, kernel_size=2)
                self.feature_maps[name] = output

            self.model.layer1.register_forward_hook(lambda m, i, o: hook_fn(m, i, o, 'layer1'))
            self.model.layer2.register_forward_hook(lambda m, i, o: hook_fn(m, i, o, 'layer2'))
            self.model.layer3.register_forward_hook(lambda m, i, o: hook_fn(m, i, o, 'layer3'))

            self.model.fc = torch.nn.Identity()
            self.model.eval()
            self.model.to(DEVICE)
            
            self.get_logger().info("âœ… PaDiM Model Loaded & Optimized on GPU!")
            
        except Exception as e:
            self.get_logger().error(f"âŒ Failed to load PaDiM: {e}")
            self.model = None

    def extract_features(self, roi_img_rgb):
        pil_img = PILImage.fromarray(roi_img_rgb)
        transform = transforms.Compose([
            transforms.Resize(256, PILImage.LANCZOS),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        input_tensor = transform(pil_img).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            _ = self.model(input_tensor)

        combined = torch.cat([self.feature_maps['layer1'], 
                              self.feature_maps['layer2'], 
                              self.feature_maps['layer3']], dim=1)
        
        patch_features = combined.permute(0, 2, 3, 1).flatten(1, 2).squeeze(0)
        patch_features = F.normalize(patch_features, p=2, dim=1)

        return patch_features

    def detect_anomaly(self, roi_img_bgr):
        if self.model is None:
            return "ERROR", 0.0

        roi_rgb = cv2.cvtColor(roi_img_bgr, cv2.COLOR_BGR2RGB)
        features = self.extract_features(roi_rgb)
        features_reduced = features[:, self.random_channels] 

        delta = features_reduced - self.mean_vector
        temp = torch.matmul(delta, self.inv_cov_matrix)
        dist_sq = torch.sum(temp * delta, dim=1)
        dist = torch.sqrt(torch.abs(dist_sq))

        top_n_values, _ = torch.topk(dist, k=min(TOP_N_PATCHES, len(dist)))
        score = torch.mean(top_n_values).item()

        quality = "DEFECT" if score > ANOMALY_THRESHOLD else "GOOD"
        return quality, score

    def undistort_point(self, u, v):
        """
        í”½ì…€ ì¢Œí‘œ (u, v) í•˜ë‚˜ë¥¼ ì…ë ¥ë°›ì•„ ë Œì¦ˆ ì™œê³¡ì„ ë³´ì •í•œ ì¢Œí‘œ (u', v')ë¥¼ ë°˜í™˜
        """
        # ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜ (1, 1, 2)
        src_pt = np.array([[[u, v]]], dtype=np.float32)
        
        # P=CAMERA_MATRIXë¥¼ ë„£ì–´ì£¼ì§€ ì•Šìœ¼ë©´ ì •ê·œí™”ëœ ì¢Œí‘œê°€ ë‚˜ì˜´.
        # ìš°ë¦¬ëŠ” ë‹¤ì‹œ í”½ì…€ ì¢Œí‘œê°€ í•„ìš”í•˜ë¯€ë¡œ Pì— ì¹´ë©”ë¼ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ë„£ì–´ì¤Œ.
        dst_pt = cv2.undistortPoints(src_pt, CAMERA_MATRIX, DIST_COEFFS, P=CAMERA_MATRIX)
        
        return dst_pt[0][0][0], dst_pt[0][0][1]

    def pixel_to_robot(self, px, py):
        """
        1. ì´ë¯¸ì§€ìƒì˜ ì (px, py)ì„ ë°›ìŒ
        2. ì™œê³¡ ë³´ì • ìˆ˜í–‰ (undistort_point)
        3. ë¡œë´‡ ì¢Œí‘œê³„ë¡œ ë³€í™˜ (perspectiveTransform)
        """
        # 1. ì™œê³¡ ë³´ì • (ë Œì¦ˆ í´ê¸°)
        undistorted_x, undistorted_y = self.undistort_point(px, py)

        # 2. ë¡œë´‡ ì¢Œí‘œ ë³€í™˜
        pt = np.array([[[undistorted_x, undistorted_y]]], dtype=np.float32)
        dst = cv2.perspectiveTransform(pt, PERSPECTIVE_MATRIX)
        
        return float(dst[0][0][0]), float(dst[0][0][1])

    def apply_clahe(self, img):
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        cl = clahe.apply(l)
        limg = cv2.merge((cl, a, b))
        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)
        return final

    def process_frame(self):
        ret, frame = self.cap.read()
        if not ret: return

        # ----------------------------------------------------------------------
        # 1. ì›ë³¸ì—ì„œ ë°”ë¡œ ROI ìë¥´ê¸° (ì†ë„ ë¹ ë¦„)
        # ----------------------------------------------------------------------
        roi = frame[ROI_Y:ROI_Y + ROI_H, ROI_X:ROI_X + ROI_W]
        roi = self.apply_clahe(roi)
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)

        # 2. ì´ˆë¡ìƒ‰ ë§ˆìŠ¤í¬
        mask = cv2.inRange(hsv, LOWER_GREEN, UPPER_GREEN)
        
        mask = cv2.dilate(mask, None, iterations=3) 
        mask = cv2.erode(mask, None, iterations=3) 
        mask = cv2.dilate(mask, None, iterations=1)

        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        msg = DetectionResult()
        
        if not cnts:
            msg.is_detected = False
            self.result_pub.publish(msg)
        
        else:
            # 3. ê°€ì¥ í° ë¬¼ì²´ ì¶”ì¶œ
            largest_contour = max(cnts, key=cv2.contourArea)
            
            if cv2.contourArea(largest_contour) < 1000:
                msg.is_detected = False
                self.result_pub.publish(msg)
            
            else:
                c = largest_contour
                rect = cv2.minAreaRect(c)
                (cx, cy), (w, h), angle = rect
                if w < h: angle += 90

                bx, by, bw, bh = cv2.boundingRect(c)
                margin = 10
                
                x1 = max(0, bx - margin)
                y1 = max(0, by - margin)
                x2 = min(ROI_W, bx + bw + margin)
                y2 = min(ROI_H, by + bh + margin)
                
                cube_img = roi[y1:y2, x1:x2]

                if cube_img.size > 0 and cube_img.shape[0] > 20 and cube_img.shape[1] > 20:
                    quality, score = self.detect_anomaly(cube_img)
                else:
                    quality, score = "UNKNOWN", 0.0

                # --------------------------------------------------------------
                # â˜… [í•µì‹¬] ROI ì¢Œí‘œ -> ì „ì²´ ì¢Œí‘œ -> ì™œê³¡ ë³´ì • -> ë¡œë´‡ ì¢Œí‘œ
                # --------------------------------------------------------------
                # (1) ROI ë‚´ë¶€ ì¤‘ì‹¬ì (cx, cy)ì„ ì „ì²´ ì´ë¯¸ì§€ ê¸°ì¤€ ì¢Œí‘œë¡œ ë³€í™˜
                global_cx = cx + ROI_X
                global_cy = cy + ROI_Y

                # (2) pixel_to_robot í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ 'undistort_point'ë¥¼ ìˆ˜í–‰í•¨
                robot_x, robot_y = self.pixel_to_robot(global_cx, global_cy)

                # ë©”ì‹œì§€ ì „ì†¡
                msg.is_detected = True
                msg.quality = quality
                msg.center = [robot_x, robot_y+100]
                msg.angle = float(angle)
                self.result_pub.publish(msg)

                # --- ì‹œê°í™” ---
                # í™”ë©´ì— ê·¸ë¦´ ë•ŒëŠ” ì™œê³¡ ë³´ì • ì „ì¸ ì›ë³¸ ì´ë¯¸ì§€(frame)ì— ê·¸ëƒ¥ ê·¸ë¦½ë‹ˆë‹¤.
                box = cv2.boxPoints(rect)
                box = np.array(box, dtype=int) 
                box[:, 0] += ROI_X
                box[:, 1] += ROI_Y
                
                color = (0, 0, 255) if quality == "DEFECT" else (0, 255, 0)
                
                cv2.drawContours(frame, [box], 0, color, 3)
                
                label = f"{quality} ({score:.1f})"
                cv2.putText(frame, label, (box[1][0], box[1][1] - 10), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                
                # ë¡œë´‡ ì¢Œí‘œ í‘œì‹œ
                coord_txt = f"X:{robot_x:.0f} Y:{robot_y:.0f}"
                cv2.putText(frame, coord_txt, (box[1][0], box[1][1] + 20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                if cube_img.size > 0: cv2.imshow("AI Input (Crop)", cube_img)

        # ROI ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        cv2.rectangle(frame, (ROI_X, ROI_Y), (ROI_X+ROI_W, ROI_Y+ROI_H), (255, 255, 0), 2)
        cv2.imshow("Vision Eye + PaDiM", frame)
        cv2.waitKey(1)

    def __del__(self):
        if self.cap.isOpened():
            self.cap.release()

def main(args=None):
    rclpy.init(args=args)
    node = VisionNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()